{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1vyoII0m7YUl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from skimage.feature import hog\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_1HCdqdOk1s",
        "outputId": "2a280e9b-634c-4a8f-9e6a-f9a88d649d1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n",
            "Downloading lfw-dataset.zip to /content\n",
            " 92% 104M/112M [00:00<00:00, 264MB/s] \n",
            "100% 112M/112M [00:00<00:00, 245MB/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "\n",
        "!pip install kaggle\n",
        "\n",
        "\n",
        "os.environ['KAGGLE_USERNAME'] = \"cruk12\"\n",
        "os.environ['KAGGLE_KEY'] = \"f6ac77148464b981fcc85de69da4347e\"\n",
        "\n",
        "\n",
        "!kaggle datasets download -d jessicali9530/lfw-dataset\n",
        "\n",
        "\n",
        "with ZipFile('lfw-dataset.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('lfw-dataset')\n",
        "\n",
        "\n",
        "os.remove('lfw-dataset.zip')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0gADFqRHIgLP"
      },
      "outputs": [],
      "source": [
        "\n",
        "def extract_features(image_path, model):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    image = preprocess(image)\n",
        "    image = image.unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        features = model(image)\n",
        "    features = features.squeeze(0)\n",
        "    return features\n",
        "\n",
        "\n",
        "def compute_hog(img):\n",
        "    resized_img = resize(img, (128*4, 64*4))\n",
        "    fd, hog_image = hog(resized_img, orientations=9, pixels_per_cell=(8, 8),\n",
        "                    cells_per_block=(2, 2), visualize=True, multichannel=True)\n",
        "    return fd\n",
        "\n",
        "\n",
        "def get_pixel(img, center, x, y):\n",
        "    new_value = 0\n",
        "    try:\n",
        "        if img[x][y] >= center:\n",
        "            new_value = 1\n",
        "    except:\n",
        "        pass\n",
        "    return new_value\n",
        "\n",
        "def lbp_calculated_pixel(img, x, y):\n",
        "    center = img[x][y]\n",
        "    val_ar = []\n",
        "    val_ar.append(get_pixel(img, center, x-1, y+1))\n",
        "    val_ar.append(get_pixel(img, center, x, y+1))\n",
        "    val_ar.append(get_pixel(img, center, x+1, y+1))\n",
        "    val_ar.append(get_pixel(img, center, x+1, y))\n",
        "    val_ar.append(get_pixel(img, center, x+1, y-1))\n",
        "    val_ar.append(get_pixel(img, center, x, y-1))\n",
        "    val_ar.append(get_pixel(img, center, x-1, y-1))\n",
        "    val_ar.append(get_pixel(img, center, x-1, y))\n",
        "\n",
        "    power_val = [1, 2, 4, 8, 16, 32, 64, 128]\n",
        "    val = 0\n",
        "    for i in range(len(val_ar)):\n",
        "        val += val_ar[i] * power_val[i]\n",
        "    return val\n",
        "\n",
        "def calcLBP(img):\n",
        "    height, width, channel = img.shape\n",
        "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    img_lbp = np.zeros((height, width,3), np.uint8)\n",
        "    for i in range(0, height):\n",
        "        for j in range(0, width):\n",
        "             img_lbp[i, j] = lbp_calculated_pixel(img_gray, i, j)\n",
        "    hist_lbp = cv2.calcHist([img_lbp], [0], None, [256], [0, 256])\n",
        "    return hist_lbp.flatten()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUycN27qIo3O",
        "outputId": "1ea5a4f7-c84d-42d6-addd-6876ad9b6ecd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 67.1MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (2): ReLU(inplace=True)\n",
              "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (5): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (6): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (7): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "resnet = models.resnet50(pretrained=True)\n",
        "resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
        "resnet.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wZjeqVQKnMcK"
      },
      "outputs": [],
      "source": [
        "lfw_folder = '/content/lfw-dataset/lfw-deepfunneled/lfw-deepfunneled'\n",
        "X, y = [], []\n",
        "for folder_name in os.listdir(lfw_folder):\n",
        "    folder_path = os.path.join(lfw_folder, folder_name)\n",
        "    if os.path.isdir(folder_path):\n",
        "        num_images = len(os.listdir(folder_path))\n",
        "        if num_images > 70:\n",
        "            for image_name in os.listdir(folder_path):\n",
        "                image_path = os.path.join(folder_path, image_name)\n",
        "                image = imread(image_path)\n",
        "                # hog_feature = compute_hog(image)\n",
        "                lbp_feature = calcLBP(image)\n",
        "                # cnn_feature = extract_features(image_path, resnet).numpy()\n",
        "                # hog_feature = hog_feature.reshape(-1)\n",
        "                lbp_feature = lbp_feature.reshape(-1)\n",
        "                # cnn_feature = cnn_feature.flatten()\n",
        "                # combined_feature = np.concatenate((hog_feature, lbp_feature, cnn_feature))\n",
        "\n",
        "                X.append(lbp_feature)\n",
        "                y.append(folder_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(X).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Myq2PBOT9xjV",
        "outputId": "6207945b-0b9a-4677-d7e1-cabd9d065e5f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1288, 256)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MCUtZ8Feo1k3"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4lxiJIBvqJY4"
      },
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "FIexW_IZMANW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy_log_reg = 0\n",
        "accuracy_log_reg_old = -1\n",
        "epochs = 10\n",
        "while ((accuracy_log_reg - accuracy_log_reg_old) > 0.01):\n",
        "  accuracy_log_reg_old = accuracy_log_reg\n",
        "  logistic_model = LogisticRegression(max_iter = epochs, multi_class='ovr')\n",
        "  logistic_model.fit(X_train, y_train_encoded)\n",
        "  y_pred_log_reg = logistic_model.predict(X_test)\n",
        "  accuracy_log_reg = accuracy_score(y_test_encoded, y_pred_log_reg)\n",
        "  epochs+= 100\n",
        "\n",
        "\n",
        "print(\"Accuracy on test data:\", accuracy_log_reg)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_encoded, y_pred_log_reg, target_names=label_encoder.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4_qU2CxM8xb",
        "outputId": "7b92850f-a0e6-45ee-dd01-f95ab6e362a2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test data: 0.5116279069767442\n",
            "Classification Report:\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "     Ariel_Sharon       0.33      0.21      0.26        14\n",
            "     Colin_Powell       0.40      0.65      0.50        40\n",
            "  Donald_Rumsfeld       0.42      0.15      0.22        34\n",
            "    George_W_Bush       0.68      0.79      0.73       109\n",
            "Gerhard_Schroeder       0.36      0.17      0.24        23\n",
            "      Hugo_Chavez       0.12      0.09      0.11        11\n",
            "       Tony_Blair       0.26      0.26      0.26        27\n",
            "\n",
            "         accuracy                           0.51       258\n",
            "        macro avg       0.37      0.33      0.33       258\n",
            "     weighted avg       0.49      0.51      0.48       258\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_log_reg = logistic_model.predict(X_train)\n",
        "accuracy_log_reg = accuracy_score(y_train_encoded, y_pred_log_reg)\n",
        "print(\"Logistic Regression\")\n",
        "print(\"Accuracy on train set:\", accuracy_log_reg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d250_4v9IciB",
        "outputId": "e16ecc5a-1e04-4a56-8b47-1b6167ba4827"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression\n",
            "Accuracy on train set: 0.6271844660194175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Grid Search to find best parameters"
      ],
      "metadata": {
        "id": "gVyAORek_kMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define hyperparameters grid\n",
        "param_grid = {\n",
        "    'penalty': ['l1', 'l2', 'none'],\n",
        "    'C': [ 0.1, 1, 10],\n",
        "    'max_iter': [100, 300, 500]\n",
        "}\n",
        "\n",
        "# Create logistic regression model\n",
        "logistic_model = LogisticRegression(multi_class='ovr')\n",
        "\n",
        "# Create GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=logistic_model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV to the data\n",
        "grid_search.fit(X_train, y_train_encoded)\n",
        "\n",
        "# Get the best estimator\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict with the best model\n",
        "y_pred_log_reg = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_log_reg = accuracy_score(y_test_encoded, y_pred_log_reg)\n",
        "\n",
        "print(\"Best parameters found by GridSearchCV:\", grid_search.best_params_)\n",
        "print(\"Accuracy on test data:\", accuracy_log_reg)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_encoded, y_pred_log_reg, target_names=label_encoder.classes_))\n"
      ],
      "metadata": {
        "id": "TTKTbFeg5phn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a7fbe99-d870-4efe-9339-4ec9c511ba3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found by GridSearchCV: {'C': 0.1, 'max_iter': 300, 'penalty': 'l2'}\n",
            "Accuracy on test data: 0.5348837209302325\n",
            "Classification Report:\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "     Ariel_Sharon       0.20      0.07      0.11        14\n",
            "     Colin_Powell       0.41      0.68      0.51        40\n",
            "  Donald_Rumsfeld       0.50      0.21      0.29        34\n",
            "    George_W_Bush       0.68      0.81      0.74       109\n",
            "Gerhard_Schroeder       0.43      0.26      0.32        23\n",
            "      Hugo_Chavez       0.22      0.18      0.20        11\n",
            "       Tony_Blair       0.35      0.26      0.30        27\n",
            "\n",
            "         accuracy                           0.53       258\n",
            "        macro avg       0.40      0.35      0.35       258\n",
            "     weighted avg       0.51      0.53      0.50       258\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Accuracy on some hyperparameter"
      ],
      "metadata": {
        "id": "SYt-9iyC-xaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the best hyperparameters obtained from grid search\n",
        "best_params = {'C': 0.1, 'max_iter': 300, 'penalty': 'l2'}\n",
        "\n",
        "# Create logistic regression model with best hyperparameters\n",
        "best_logistic_model = LogisticRegression(**best_params, multi_class='ovr')\n",
        "\n",
        "# Fit the model on training data\n",
        "best_logistic_model.fit(X_train, y_train_encoded)\n",
        "\n",
        "# Predict on training data\n",
        "y_pred_train = best_logistic_model.predict(X_train)\n",
        "\n",
        "# Calculate training accuracy\n",
        "accuracy_train = accuracy_score(y_train_encoded, y_pred_train)\n",
        "\n",
        "print(\"Training Accuracy with best hyperparameters:\", accuracy_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHOov3Wa-xCu",
        "outputId": "a8ff3f17-d0b0-4a2d-e073-ca4c8d3a084c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy with best hyperparameters: 0.625242718446602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA on logistic regression"
      ],
      "metadata": {
        "id": "L2_r15QI_tmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=0.95)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "logistic_model.fit(X_train_pca, y_train_encoded)\n",
        "y_pred_log_reg = logistic_model.predict(X_test_pca)\n",
        "accuracy = accuracy_score(y_test_encoded, y_pred_log_reg)\n",
        "print(\"PCA + Logistic Regression Classifier\")\n",
        "print(\"Accuracy on test set:\", accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_encoded, y_pred_log_reg, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJ0WLTDdEvZS",
        "outputId": "46a3c158-5c25-42cd-8f65-9742baa43da5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PCA + Logistic Regression Classifier\n",
            "Accuracy on test set: 0.4069767441860465\n",
            "Classification Report:\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "     Ariel_Sharon       0.00      0.00      0.00        14\n",
            "     Colin_Powell       0.29      0.28      0.28        40\n",
            "  Donald_Rumsfeld       0.00      0.00      0.00        34\n",
            "    George_W_Bush       0.43      0.86      0.57       109\n",
            "Gerhard_Schroeder       0.00      0.00      0.00        23\n",
            "      Hugo_Chavez       0.00      0.00      0.00        11\n",
            "       Tony_Blair       0.00      0.00      0.00        27\n",
            "\n",
            "         accuracy                           0.41       258\n",
            "        macro avg       0.10      0.16      0.12       258\n",
            "     weighted avg       0.23      0.41      0.29       258\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_log_reg = logistic_model.predict(X_train_pca)\n",
        "accuracy_log_reg = accuracy_score(y_train_encoded, y_pred_log_reg)\n",
        "print(\"PCA + Logistic Regression Classifier\")\n",
        "print(\"Accuracy on train set:\", accuracy_log_reg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGhVnHZMIJVW",
        "outputId": "cbb5b0ae-9e97-494a-994e-b79d361c963d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PCA + Logistic Regression Classifier\n",
            "Accuracy on train set: 0.429126213592233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LDA on logistic regression"
      ],
      "metadata": {
        "id": "UMnG1Upo_51i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "lda = LDA(n_components=None)\n",
        "X_train_lda = lda.fit_transform(X_train, y_train_encoded)\n",
        "X_test_lda = lda.transform(X_test)\n",
        "logistic_model.fit(X_train_lda, y_train_encoded)\n",
        "y_pred_log_reg = logistic_model.predict(X_test_lda)\n",
        "accuracy_gnb = accuracy_score(y_test_encoded, y_pred_log_reg)\n",
        "print(\"LDA + Logistic Regression\")\n",
        "print(\"Accuracy on Test set after LDA:\", accuracy_gnb)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_encoded, y_pred_log_reg, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ssBhAGSGIFX",
        "outputId": "0adff294-d0f5-46df-ae6f-6136045fc3f5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LDA + Logistic Regression\n",
            "Accuracy on Test set after LDA: 0.42248062015503873\n",
            "Classification Report:\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "     Ariel_Sharon       0.31      0.29      0.30        14\n",
            "     Colin_Powell       0.37      0.55      0.44        40\n",
            "  Donald_Rumsfeld       0.27      0.12      0.16        34\n",
            "    George_W_Bush       0.60      0.64      0.62       109\n",
            "Gerhard_Schroeder       0.38      0.22      0.28        23\n",
            "      Hugo_Chavez       0.00      0.00      0.00        11\n",
            "       Tony_Blair       0.12      0.15      0.13        27\n",
            "\n",
            "         accuracy                           0.42       258\n",
            "        macro avg       0.29      0.28      0.28       258\n",
            "     weighted avg       0.41      0.42      0.41       258\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_log_reg = logistic_model.predict(X_train_lda)\n",
        "accuracy_log_reg = accuracy_score(y_train_encoded, y_pred_log_reg)\n",
        "print(\"LDA + Logistic Regression\")\n",
        "print(\"Accuracy on Train set:\", accuracy_log_reg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcYelH-fHyBT",
        "outputId": "f69ae8ff-ddd5-4b23-dddd-3d87bb17db21"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LDA + Logistic Regression\n",
            "Accuracy on Train set: 0.7718446601941747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Grid Search to find best parameters"
      ],
      "metadata": {
        "id": "FI0jiXO1BlGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create logistic regression model\n",
        "logistic_model = LogisticRegression(multi_class='ovr')\n",
        "\n",
        "# Define hyperparameters grid\n",
        "param_grid = {\n",
        "    'penalty': ['l1', 'l2', 'none'],\n",
        "    'C': [0.1, 1, 10],\n",
        "    'max_iter': [100, 300, 500]\n",
        "}\n",
        "\n",
        "# Create GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=logistic_model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV to the data\n",
        "grid_search.fit(X_train_lda, y_train_encoded)\n",
        "\n",
        "# Get the best estimator\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict with the best model on test data\n",
        "y_pred_log_reg = best_model.predict(X_test_lda)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_log_reg = accuracy_score(y_test_encoded, y_pred_log_reg)\n",
        "\n",
        "print(\"Best parameters found by GridSearchCV:\", grid_search.best_params_)\n",
        "print(\"Accuracy on test data after LDA:\", accuracy_log_reg)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_encoded, y_pred_log_reg, target_names=label_encoder.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zwQ6fg7ATNi",
        "outputId": "a3906510-662a-4035-ba24-55c830f50a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found by GridSearchCV: {'C': 0.1, 'max_iter': 100, 'penalty': 'l2'}\n",
            "Accuracy on test data after LDA: 0.43410852713178294\n",
            "Classification Report:\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "     Ariel_Sharon       0.33      0.29      0.31        14\n",
            "     Colin_Powell       0.37      0.55      0.44        40\n",
            "  Donald_Rumsfeld       0.29      0.12      0.17        34\n",
            "    George_W_Bush       0.61      0.66      0.63       109\n",
            "Gerhard_Schroeder       0.46      0.26      0.33        23\n",
            "      Hugo_Chavez       0.00      0.00      0.00        11\n",
            "       Tony_Blair       0.12      0.15      0.14        27\n",
            "\n",
            "         accuracy                           0.43       258\n",
            "        macro avg       0.31      0.29      0.29       258\n",
            "     weighted avg       0.42      0.43      0.42       258\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Accuracy on some hyperparameter"
      ],
      "metadata": {
        "id": "VnSGgWE9Beuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the best hyperparameters obtained from grid search\n",
        "best_params = {'C': 0.1, 'max_iter': 100, 'penalty': 'l2'}\n",
        "\n",
        "# Create logistic regression model with best hyperparameters\n",
        "best_logistic_model = LogisticRegression(**best_params, multi_class='ovr')\n",
        "\n",
        "# Fit the model on training data\n",
        "best_logistic_model.fit(X_train_lda, y_train_encoded)\n",
        "\n",
        "# Predict on training data\n",
        "y_pred_train = best_logistic_model.predict(X_train_lda)\n",
        "\n",
        "# Calculate training accuracy\n",
        "accuracy_train = accuracy_score(y_train_encoded, y_pred_train)\n",
        "\n",
        "print(\"Training Accuracy with best hyperparameters:\", accuracy_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fRLHN_fAgSQ",
        "outputId": "c3f26ddd-d1ee-4cbd-ee4b-aeba3ff5486f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy with best hyperparameters: 0.7737864077669903\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rvGlE4RiBRN6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}